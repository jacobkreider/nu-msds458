{"cells":[{"cell_type":"markdown","source":["*Notebook Created by: Jacob Kreider*\n","2019-01-18\n","### Notes before proceeding:\n","The following notebook contains code written by Dr. A.J. Maren for the\n","2019 Winter term Deep Learning class in the MSDS program at Northwestern\n","University. All comments wrapped in single-line or inline quotation marks,\n","except where specified, are hers. Any editing of those comments is done\n","for the sole purpose of displaying the contents in a Jupyter-friendly format.\n","\n","Non-quoted commentary and code comments are mine alone. No code has been\n","changed in any way.\n","\n","I will, however, remove placeholder comments and other non-essential\n","text when I feel its absence makes the tutorial clearer for me to\n","follow. For the complete, unedited version, see the original file.\n","#### Start of copied script:"],"metadata":{}},{"cell_type":"markdown","source":["#### Initial imports"],"metadata":{}},{"source":["import random # We'll be defining our initial connection weight randomly,\n","              # and randomly selecting training data\n","from math import exp # We'll use the exp function (e^x) as part of our \n","                     # transfer function\n","import numpy as np   # For our arrays\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["##### \"\n","This is a tutorial program, designed for those who are learning Python,\n","and specifically using Python for neural networks applications.\n","\n","#### SPECIAL NOTE re/ Python code structure:\n","Python uses a 'main' module, which is typically located at the end of the code.\n","There is a short line after that which actual RUNS the 'main' module.\n","All supporting tasks are defined as various procedures and functions.\n","They are stored higher in the code.\n","\n","Typically, the most general and global procedures and functions are at\n","the bottom of the code, and the more detail-specific ones are at the top.\n","Thus, if you want to understand a piece of code, you might want to read\n","from bottom-to-top (or from more general-to-specific), instead of top-\n","to-bottom (detailed-specific-to-general).\n","\n","Notice that control (e.g., nesting) is defined by spacing.\n","You can define the number of spaces for a tab, but you have to be consistent.\n","Notice that once a procedure is defined, every command within it must\n","be tabbed in.\n","##### \""],"metadata":{}},{"cell_type":"markdown","source":["#### Procedure to welcome the user and identify the code\n","This procedure is called from the 'main' program.\n","Notice that it has an empty parameter list.\n","Procedures require a parameter list, but it can be empty."],"metadata":{}},{"source":["def welcome(): # The parameter list here is empty, which is ok. \n","               # Every function needs a list, even if it is an empty one.\n","    print()\n","    print('*******************************************************************')\n","    print()\n","    print('Welcome to the Multilayer Perceptron Neural Network')\n","    print('  trained using the backpropagation method.')\n","    print('Version 0.2, 01/10/2017, A.J. Maren')\n","    print('For comments, questions, or bug-fixes,') \n","    print('contact: alianna.maren@northwestern.edu')\n","    print()\n","    print('*******************************************************************')\n","    print()\n","    return() # This statement returns us to 'main'. The parameter list is\n","             # still empty, as the function did not specify anything to \n","             # put in it.\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### A collection of worker-functions, designed to do specific small tasks\n","Here, we will define to functions:\n","* \"Compute neuron activation using sigmoid transfer function\"\n","* \"Compute derivative of transfer function\""],"metadata":{}},{"source":["# \"Compute neuron activation using sigmoid transfer function\"\n","def computeTransferFnctn(summedNeuronInput, alpha):\n","    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput)) \n","    return activation   \n","\n","# \"Compute derivative of transfer function\"\n","def computeTransferFnctnDeriv(NeuronOutput, alpha):\n","    return alpha*NeuronOutput*(1.0 -NeuronOutput)  \n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### Procedure to obtain the neural network size specifications\n","The below procedure is called from 'main' and operates as a function.\n","It returns a list of three values (the list is technically a single\n","value).\n","\n","The purpose here is to allow the user to \"specify the size of the input\n","(I), hidden (H), and output (O) layers.\" The object 'arraySizeList' will\n","store these three values for us.\n","\n","This list is the basis for the sizes of two different weight arrays:\n","* wWeights (the Input-to-Hidden array); and,\n","* vWeights (the Hidden-to-Output array)\n","\n","*Note: For this tutorial, we're hard-coding those weight array sizes.*"],"metadata":{}},{"source":["# \"Procedure to obtain the neural network size specifications\"\n","def obtainNeuralNetworkSizeSpecs ():\n","    numInputNodes = 2\n","    numHiddenNodes = 2\n","    numOutputNodes = 2   \n","    print(' ')\n","    print('This network is set up to run the X-OR problem.')\n","    print('The numbers of nodes in the input, hidden, and output layers have')\n","    print('been set to 2 each.')\n","    # Create an list containing these needed array sizes\n","    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes) \n","    return (arraySizeList) #Return this list to 'main'\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### \"Function to initialize a specific connection weight with a randomly-generated number between 0 & 1\""],"metadata":{}},{"source":["def InitializeWeight ():\n","\n","    randomNum = random.random()\n","    weight=1-2*randomNum\n","#    print(weight)\n","           \n","    return (weight)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### Procedure to initialize the connection weight arrays\n","##### \"\n","This procedure is also called directly from 'main.'\n","\n","This procedure takes in the two parameters, the number of nodes on the\n","bottom (of any two layers), and the number of nodes in the layer just\n","above it. It will use these two sizes to create a weight array.\n","\n","The weights will initially be given assigned values here, so that we\n","can trace the creation and transfer of this array back to the 'main'\n","procedure. Later, we will randomly define initial weight values.\n","\n","These values will be stored in an array, the weightArray.\n","\n","This procedure is being called as a function; the returned value is the\n","new connection weight matrix.\n","\n","Right now, for simplicity and test purposes, the weightArray is set to\n","a single value.\n","\n","Note my variable-naming convention:\n","* If it is an array, I call it variableNameArray\n","* If it is a list, I call it variableNameList\n","* It's a lot like calling your dog Roger \"rogerDog\"\n","and your cat Fluffy \"fluffyCat\"\n","but until we're better at telling cats from dogs, this helps.\n","##### \""],"metadata":{}},{"source":["# \"Procedure to initialize the connection weight arrays\"\n","def initializeWeightArray (weightArraySizeList, debugInitializeOff):\n","    numBottomNodes = weightArraySizeList[0] # This is unused? Why is it here?\n","    numUpperNodes = weightArraySizeList[1]  # This is unused? Why is it here? \n","    # Initialize the weight variables with random weights\n","    wt00=InitializeWeight ()\n","    wt01=InitializeWeight ()\n","    wt10=InitializeWeight ()\n","    wt11=InitializeWeight ()    \n","    weightArray=np.array([[wt00,wt10],[wt01,wt11]])\n","\n","    # \"Debug mode: if debug is set to False, then we DO NOT do the prints\"\n","    if not debugInitializeOff:\n","       # \"Print the weights\"\n","        print(' ')\n","        print('  Inside initializeWeightArray')\n","        print('    The weights just initialized are: ')\n","        print('      weight00 = %.4f,' % wt00)\n","        print('      weight01 = %.4f,' % wt01)\n","        print('      weight10 = %.4f,' % wt10)\n","        print('      weight11 = %.4f,' % wt11)  \n","\n","    # \"Debug mode: if debug is set to False, then we DO NOT do the prints\"\n","    if not debugInitializeOff:\n","    # \"Print the entire weights array: \"\n","        print(' ')\n","        print('    The weight Array just established is: ', weightArray)\n","        print(' ') \n","        print('    Within this array: ')\n","        print('      weight00 = %.4f    weight10 = %.4f' % (weightArray[0,0], weightArray[0,1]))\n","        print('      weight01 = %.4f    weight11 = %.4f' % (weightArray[1,0], weightArray[1,1]))    \n","        print('  Returning to calling procedure')\n","\n","    return (weightArray) # Return the created weight array to 'main'\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["##### Some notes on the above procedure:\n","##### \"\n","The weight array is set up so that it lists the rows, and within the rows, the columns\n","wt00   wt10\n","wt10   wt11\n","\n","\n","The sum of weighted terms should be carried out as follows:\n","\n","`[wt 00  wt10]` * `[node0]` = wt00*node0 + wt10*node1 = sum-weighted-nodes-to-higher-node0\n","`[wt 10  wt11]` * `[node1]` = wt10*node0 + wt11*node1 = sum-weighted-nodes-to-higher-node1\n","\n","Notice that the weight positions are being labeled according to how\n","Python numbers elements in an array... so, the first one is in position `[0,0]`.\n","\n","Notice that the position of the weights in the weightArray is not as would be expected:\n","\n","wt00 = weight connecting 0th lower-level node to 0th upper-level node = weightArray `[0,0]`<br/>\n","wt10 = weight connecting 1st lower-level node to 0th upper-level node = weightArray `[0,1]`<br/>\n","wt01 = weight connecting 0th lower-level node to 1st upper-level node = weightArray `[1,0]`<br/>\n","wt11 = weight connecting 1st lower-level node to 1st upper-level node = weightArray `[1,1]`<br/>\n","\n","Notice that wt01 & wt10 are reversed from what we'd expect\n","##### \"\n","<br/>\n","\n","#### Function to initialize the bias weight arrays\n","\n","This procedure follows the same basic idea as the one above. It initializes\n","weight variables with random wights and outputs an array to hold them.\n","The difference here is that this array holds the *bias* weights, not the\n","weights between each node connection."],"metadata":{}},{"source":["# Function to initialize the bias weight arrays\n","def initializeBiasWeightArray (weightArray1DSize):\n","    numBiasNodes = weightArray1DSize # This is a placeholder step for now.\n","                                     # Once we start using array operations,\n","                                     # It will be more important\n","    # Initialize the weight variables with random weights\n","    biasWeight0=InitializeWeight ()\n","    biasWeight1=InitializeWeight ()\n","      \n","    biasWeightArray=np.array([biasWeight0,biasWeight1]) \n","\n","    return (biasWeightArray)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### Function to build random training data\n","* Training data lists will contain:\n","* - Two input values (0 or 1)\n","* - Two output values (o or 1)\n","* - A value equal to the number of the random-chosen dataset\n","\n","The dataset number is included so that we can calculate the SSE against\n","the appropriate dataset. We can't stop the training until all the SSEs\n","are below a predefined minimum."],"metadata":{}},{"source":["# Function to build random training data\n","def obtainRandomXORTrainingValues ():\n","    trainingDataSetNum = random.randint(1, 4)\n","    if trainingDataSetNum >1.1: # The selection is for training lists 2-4\n","        if trainingDataSetNum > 2.1: # The selection is for training lists 3 & 4\n","            if trainingDataSetNum > 3.1: # The selection is for training list 4\n","                trainingDataList = (1,1,0,1,3) # training data list 4 selected\n","            else: trainingDataList = (1,0,1,0,2) # training data list 3 selected   \n","        else: trainingDataList = (0,1,1,0,1) # training data list 2 selected     \n","    else: trainingDataList = (0,0,0,1,0) # training data list 1 selected \n","           \n","    return (trainingDataList) \n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### \"Compute neuron activation\"\n","\"this is the summed weighted inputs after passing through transfer fnctn\""],"metadata":{}},{"source":["def computeSingleNeuronActivation(alpha, wt0, wt1, input0, input1, bias, \n","                                  debugComputeSingleNeuronActivationOff):\n","    # Obtain the inputs into the neuron (the sum of weights times inputs)\n","    summedNeuronInput = wt0*input0+wt1*input1+bias\n","    # Pass the above result and the transfer function parameter (alpha)\n","    # into the transfer function\n","    activation = computeTransferFnctn(summedNeuronInput, alpha)\n","\n","    if not debugComputeSingleNeuronActivationOff:        \n","        print(' ')\n","        print('  In computeSingleNeuronActivation with input0,') \n","        print('  input 1 given as: ', input0, ', ', input1)\n","        print('    The summed neuron input is %.4f' % summedNeuronInput)\n","        print('    The activation (applied transfer function) for') \n","        print('    that neuron is %.4f' % activation)\n","    return activation\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### Perform a single feedforward pass\n","A reminder from Dr. Maren on assigning the weights from the weight arrays:\n","##### \"\n","Recall from InitializeWeightArray:\n","Notice that the position of the weights in the weightArray is not as would be expected:\n","\n","wt00 = weight connecting 0th lower-level node to 0th upper-level node = weightArray `[0,0]`<br/>\n","wt10 = weight connecting 1st lower-level node to 0th upper-level node = weightArray `[0,1]`<br/>\n","wt01 = weight connecting 0th lower-level node to 1st upper-level node = weightArray `[1,0]`<br/>\n","wt11 = weight connecting 1st lower-level node to 1st upper-level node = weightArray `[1,1]`<br/>\n","\n","Notice that wt01 & wt10 are reversed from what we'd expect\n","##### \""],"metadata":{}},{"source":["def ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, \n","                                  vWeightArray, biasHiddenWeightArray, \n","                                  biasOutputWeightArray, \n","                                  debugComputeSingleFeedforwardPassOff):\n","    input0 = inputDataList[0]\n","    input1 = inputDataList[1]\n","\n","    # Assign the input-to-hidden weights to specific variables\n","    wWt00 = wWeightArray[0,0]\n","    wWt10 = wWeightArray[0,1]\n","    wWt01 = wWeightArray[1,0]       \n","    wWt11 = wWeightArray[1,1] # See note in above comments on these assignments\n","\n","    # Assign the hidden-to-output weights to specific variables\n","    vWt00 = vWeightArray[0,0]\n","    vWt10 = vWeightArray[0,1]\n","    vWt01 = vWeightArray[1,0]       \n","    vWt11 = vWeightArray[1,1] # See note in above comments on these assignments\n","\n","    biasHidden0 = biasHiddenWeightArray[0]\n","    biasHidden1 = biasHiddenWeightArray[1]\n","    biasOutput0 = biasOutputWeightArray[0]\n","    biasOutput1 = biasOutputWeightArray[1]\n","\n","    # Obtain the activations of the hidden nodes  \n","    if not debugComputeSingleFeedforwardPassOff:\n","        debugComputeSingleNeuronActivationOff = False\n","    else: \n","        debugComputeSingleNeuronActivationOff = True\n","        \n","    if not debugComputeSingleNeuronActivationOff:\n","        print(' ')\n","        print('  For hiddenActivation0 from input0, input1 = ',input0,', ',input1)\n","    \n","    hiddenActivation0 = computeSingleNeuronActivation(alpha, wWt00, wWt10, input0,\n","                 input1, biasHidden0, debugComputeSingleNeuronActivationOff) \n","   \n","    if not debugComputeSingleNeuronActivationOff:\n","        print(' ')\n","        print('  For hiddenActivation1 from input0, input1 = ',input0,', ',input1)    \n","   \n","    hiddenActivation1 = computeSingleNeuronActivation(alpha, wWt01, wWt11, input0,\n","                 input1, biasHidden1, debugComputeSingleNeuronActivationOff)\n","\n","    if not debugComputeSingleFeedforwardPassOff: \n","        print(' ')\n","        print('In computeSingleFeedforwardPass: ')\n","        print('Input node values: ', input0, ', ', input1)\n","        print('The activations for the hidden nodes are:')\n","        print('Hidden0 = %.4f' % hiddenActivation0) \n","        print('Hidden1 = %.4f' % hiddenActivation1)\n","\n","    # Obtain the activations of the output nodes    \n","    outputActivation0 = computeSingleNeuronActivation(alpha, vWt00, vWt10, \n","            hiddenActivation0, hiddenActivation1, biasOutput0, \n","            debugComputeSingleNeuronActivationOff)\n","    outputActivation1 = computeSingleNeuronActivation(alpha, vWt01, vWt11, \n","            hiddenActivation0, hiddenActivation1, biasOutput1, \n","            debugComputeSingleNeuronActivationOff)\n","\n","    if not debugComputeSingleFeedforwardPassOff: \n","        print(' ')\n","        print('  Computing the output neuron activations')\n","        print(' ')\n","        print('Back in ComputeSingleFeedforwardPass(for hidden-to-output computations)')\n","        print('  The activations for the output nodes are:')\n","        print('Output0 = %.4f' % outputActivation0)\n","        print('Output1 = %.4f' % outputActivation1)\n","\n","    actualAllNodesOutputList = (hiddenActivation0, hiddenActivation1, \n","                                outputActivation0, outputActivation1)\n","                                                                                                \n","    return (actualAllNodesOutputList)    \n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["##### \"Determine initial and Total Sum Squared Errors\"\n","This function will return an array containing:\n","* The SSE of each of our training data sets plus the total\n","\n","Note that this function calls ComputeSingleFeedforwardPass, which we\n","just created above"],"metadata":{}},{"source":["def computeSSE_Values (alpha, SSE_InitialArray, wWeightArray, vWeightArray, \n","                biasHiddenWeightArray, biasOutputWeightArray, \n","                debugSSE_InitialComputationOff): \n","    if not debugSSE_InitialComputationOff:\n","        debugComputeSingleFeedforwardPassOff = False\n","    else: \n","        debugComputeSingleFeedforwardPassOff = True\n","    \n","    # \"Compute a single feed-forward pass & obtain the Actual Outputs \n","    # for ZEROTH data set\"\n","    inputDataList = (0, 0)           \n","    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, \n","                               wWeightArray, vWeightArray, biasHiddenWeightArray, \n","                               biasOutputWeightArray, \n","                               debugComputeSingleFeedforwardPassOff)        \n","    actualOutput0 = actualAllNodesOutputList [2]\n","    actualOutput1 = actualAllNodesOutputList [3] \n","    error0 = 0.0 - actualOutput0\n","    error1 = 1.0 - actualOutput1\n","    SSE_InitialArray[0] = error0**2 + error1**2\n","\n","    # \"debug print for function\":\n","    if not debugSSE_InitialComputationOff:\n","        print(' ')\n","        print('  In computeSSE_Values')\n","\n","    # \"debug print for (0,0)\"\":\n","    if not debugSSE_InitialComputationOff: \n","        input0 = inputDataList [0]\n","        input1 = inputDataList [1]\n","        print(' ')\n","        print('Actual Node Outputs for (0,0) training set:')\n","        print('input0 = ', input0, '   input1 = ', input1)\n","        print('actualOutput0 = %.4f   actualOutput1 = %.4f' %(actualOutput0, actualOutput1))\n","        print('error0 =        %.4f   error1 =        %.4f' %(error0, error1))\n","        print('Initial SSE for (0,0) = %.4f' % SSE_InitialArray[0])\n","\n","    # \"Compute a single feed-forward pass & obtain the Actual Outputs \n","    # for FIRST data set\"\n","    inputDataList = (0, 1)           \n","    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, \n","                               wWeightArray, vWeightArray, biasHiddenWeightArray, \n","                               biasOutputWeightArray, \n","                               debugComputeSingleFeedforwardPassOff)        \n","    actualOutput0 = actualAllNodesOutputList [2]\n","    actualOutput1 = actualAllNodesOutputList [3] \n","    error0 = 0.0 - actualOutput0\n","    error1 = 1.0 - actualOutput1\n","    SSE_InitialArray[1] = error0**2 + error1**2\n","\n","    # \"debug print for (0,1)\":\n","    if not debugSSE_InitialComputationOff: \n","        input0 = inputDataList [0]\n","        input1 = inputDataList [1]\n","        print(' ')\n","        print('Actual Node Outputs for (0,1) training set:')\n","        print('input0 = ', input0, '   input1 = ', input1)\n","        print('actualOutput0 = %.4f   actualOutput1 = %.4f' %(actualOutput0, actualOutput1))\n","        print('error0 =        %.4f   error1 =        %.4f' %(error0, error1))\n","        print('Initial SSE for (0,1) = %.4f' % SSE_InitialArray[1])\n","\n","    # \"Compute a single feed-forward pass & obtain the Actual Outputs \n","    # for SECOND data set\"\n","    inputDataList = (1, 0)           \n","    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, \n","                               wWeightArray, vWeightArray, biasHiddenWeightArray, \n","                               biasOutputWeightArray, \n","                               debugComputeSingleFeedforwardPassOff)        \n","    actualOutput0 = actualAllNodesOutputList [2]\n","    actualOutput1 = actualAllNodesOutputList [3] \n","    error0 = 0.0 - actualOutput0\n","    error1 = 1.0 - actualOutput1\n","    SSE_InitialArray[2] = error0**2 + error1**2\n","\n","    # \"debug print for (1,0):\"\n","    if not debugSSE_InitialComputationOff: \n","        input0 = inputDataList [0]\n","        input1 = inputDataList [1]\n","        print(' ')\n","        print('Actual Node Outputs for (1,0) training set:')\n","        print('input0 = ', input0, '   input1 = ', input1)\n","        print('actualOutput0 = %.4f   actualOutput1 = %.4f' %(actualOutput0, actualOutput1))\n","        print('error0 =        %.4f   error1 =        %.4f' %(error0, error1))\n","        print('Initial SSE for (1,0) = %.4f' % SSE_InitialArray[2])\n","\n","    # \"Compute a single feed-forward pass & obtain the Actual Outputs \n","    # for THIRD data set\"\n","    inputDataList = (1, 1)           \n","    actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, \n","                               wWeightArray, vWeightArray, biasHiddenWeightArray, \n","                               biasOutputWeightArray, \n","                               debugComputeSingleFeedforwardPassOff)        \n","    actualOutput0 = actualAllNodesOutputList [2]\n","    actualOutput1 = actualAllNodesOutputList [3] \n","    error0 = 0.0 - actualOutput0\n","    error1 = 1.0 - actualOutput1\n","    SSE_InitialArray[3] = error0**2 + error1**2\n","\n","    # \"debug print for (1,1):\"\n","    if not debugSSE_InitialComputationOff: \n","        input0 = inputDataList [0]\n","        input1 = inputDataList [1]\n","        print(' ')\n","        print('Actual Node Outputs for (1,1) training set:')\n","        print('input0 = ', input0, '   input1 = ', input1)\n","        print('actualOutput0 = %.4f   actualOutput1 = %.4f' %(actualOutput0, actualOutput1))\n","        print('error0 =        %.4f   error1 =        %.4f' %(error0, error1))\n","        print('Initial SSE for (1,1) = %.4f' % SSE_InitialArray[3])\n","\n","    # \"Initialize an array of SSE values\"\n","    SSE_InitialTotal = (SSE_InitialArray[0] + SSE_InitialArray[1] \n","                        + SSE_InitialArray[2] + SSE_InitialArray[3]) \n","\n","    # \"debug print for SSE_InitialTotal:\"\n","    if not debugSSE_InitialComputationOff: \n","        print(' ')\n","        print('  The initial total of the SSEs is %.4f' %SSE_InitialTotal)\n","\n","    SSE_InitialArray[4] = SSE_InitialTotal\n","    \n","    return SSE_InitialArray\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["## \"Backpropagation Section\"\n","\n","### \"Optional Debug and Code-Trace Print\"\n","<br/><br/>\n","\n","The two functions below will print the hidden node and output node\n","activations, as well as the transfer function derivatives.\n","<br/><br/>\n","The first function will then compute the deltas at each weight from eta, error, the\n","transfer function derivative, and the hidden node activation. Finally,\n","it will print the hidden-to-output connection weights.\n","<br/><br/>\n","The second function will do the same, but the computation for the deltas\n","will use the imput and a \"SumTerm for given H\" instead of the hidden\n","node activation.\n","<br/><br/>\n","#### Backpropagate the hidden-to-output connection weights"],"metadata":{}},{"source":["# Code-Trace Print: Hidden-to-Output\n","def PrintAndTraceBackpropagateOutputToHidden (alpha, eta, errorList, \n","                                              actualAllNodesOutputList, \n","                                              transFuncDerivList, deltaVWtArray, \n","                                              vWeightArray, newVWeightArray):   \n","    hiddenNode0 = actualAllNodesOutputList[0]\n","    hiddenNode1 = actualAllNodesOutputList[1]\n","    outputNode0 = actualAllNodesOutputList[2]    \n","    outputNode1 = actualAllNodesOutputList[3]\n","    transFuncDeriv0 = transFuncDerivList[0]\n","    transFuncDeriv1 = transFuncDerivList[1]\n","\n","    deltaVWt00 = deltaVWtArray[0,0]\n","    deltaVWt01 = deltaVWtArray[1,0]\n","    deltaVWt10 = deltaVWtArray[0,1]\n","    deltaVWt11 = deltaVWtArray[1,1]    \n","    \n","    error0 = errorList[0]\n","    error1 = errorList[1]\n","\n","    print(' ')\n","    print('In Print and Trace for Backpropagation: Hidden to Output Weights')\n","    print('  Assuming alpha = 1')\n","    print(' ')\n","    print('  The hidden node activations are:')\n","    print('    Hidden node 0: ', '  %.4f' % hiddenNode0, '  Hidden node 1: ', '  %.4f' % hiddenNode1)\n","    print(' ')\n","    print('  The output node activations are:')\n","    print('    Output node 0: ', '  %.3f' % outputNode0, '   Output node 1: ', '  %.3f' % outputNode1)\n","    print(' ')\n","    print('  The transfer function derivatives are: ')\n","    print('    Deriv-F(0): ', '     %.3f' % transFuncDeriv0, '   Deriv-F(1): ', '     %.3f' % transFuncDeriv1)\n","\n","    print(' ') \n","    print('The computed values for the deltas are: ')\n","    print('                eta  *  error  *   trFncDeriv *   hidden')\n","    print('  deltaVWt00 = ',' %.2f' % eta, '* %.4f' % error0, ' * %.4f' % transFuncDeriv0, '  * %.4f' % hiddenNode0)\n","    print('  deltaVWt01 = ',' %.2f' % eta, '* %.4f' % error1, ' * %.4f' % transFuncDeriv1, '  * %.4f' % hiddenNode0)                      \n","    print('  deltaVWt10 = ',' %.2f' % eta, '* %.4f' % error0, ' * %.4f' % transFuncDeriv0, '  * %.4f' % hiddenNode1)\n","    print('  deltaVWt11 = ',' %.2f' % eta, '* %.4f' % error1, ' * %.4f' % transFuncDeriv1, '  * %.4f' % hiddenNode1)\n","    print(' ')\n","    print('Values for the hidden-to-output connection weights:')\n","    print('           Old:     New:      eta*Delta:')\n","    print('[0,0]:   %.4f' % vWeightArray[0,0], '  %.4f' % newVWeightArray[0,0], '  %.4f' % deltaVWtArray[0,0])\n","    print('[0,1]:   %.4f' % vWeightArray[1,0], '  %.4f' % newVWeightArray[1,0], '  %.4f' % deltaVWtArray[1,0])\n","    print('[1,0]:   %.4f' % vWeightArray[0,1], '  %.4f' % newVWeightArray[0,1], '  %.4f' % deltaVWtArray[0,1])\n","    print('[1,1]:   %.4f' % vWeightArray[1,1], '  %.4f' % newVWeightArray[1,1], '  %.4f' % deltaVWtArray[1,1])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### \"Backpropagate the input-to-hidden connection weights\""],"metadata":{}},{"source":["# \"Code-Trace Print: Backpropagate the input-to-hidden connection weights\"\n","def PrintAndTraceBackpropagateHiddenToInput (alpha, eta, inputDataList, errorList, \n","                                             actualAllNodesOutputList, \n","                                             transFuncDerivHiddenList, \n","                                             transFuncDerivOutputList, \n","                                             deltaWWtArray, vWeightArray, \n","                                             wWeightArray, newWWeightArray, \n","                                             biasWeightArray):\n","\n","    inputNode0 = inputDataList[0]\n","    inputNode1 = inputDataList[1]    \n","    hiddenNode0 = actualAllNodesOutputList[0]\n","    hiddenNode1 = actualAllNodesOutputList[1]\n","    outputNode0 = actualAllNodesOutputList[2]    \n","    outputNode1 = actualAllNodesOutputList[3]\n","    transFuncDerivHidden0 = transFuncDerivHiddenList[0]\n","    transFuncDerivHidden1 = transFuncDerivHiddenList[1]\n","    transFuncDerivOutput0 = transFuncDerivOutputList[0]\n","    transFuncDerivOutput1 = transFuncDerivOutputList[1] \n","\n","    wWt00 = wWeightArray[0,0]\n","    wWt01 = wWeightArray[1,0]\n","    wWt10 = wWeightArray[0,1]       \n","    wWt11 = wWeightArray[1,1]\n","    \n","    deltaWWt00 = deltaWWtArray[0,0]\n","    deltaWWt01 = deltaWWtArray[1,0]\n","    deltaWWt10 = deltaWWtArray[0,1]\n","    deltaWWt11 = deltaWWtArray[1,1] \n","    \n","    vWt00 = vWeightArray[0,0]\n","    vWt01 = vWeightArray[1,0]\n","    vWt10 = vWeightArray[0,1]\n","    vWt11 = vWeightArray[1,1]                \n","    \n","    error0 = errorList[0]\n","    error1 = errorList[1]\n","\n","    errorTimesTransD0 = error0*transFuncDerivOutput0\n","    errorTimesTransD1 = error1*transFuncDerivOutput1    \n","            \n","    biasHidden0 = biasWeightArray[0,0]\n","    biasHidden1 = biasWeightArray[0,1]                                      \n","\n","    partialSSE_w_Wwt00 = -transFuncDerivHidden0*inputNode0*(vWt00*error0 + vWt01*error1)                                                             \n","    partialSSE_w_Wwt01 = -transFuncDerivHidden1*inputNode0*(vWt10*error0 + vWt11*error1)\n","    partialSSE_w_Wwt10 = -transFuncDerivHidden0*inputNode1*(vWt00*error0 + vWt01*error1)\n","    partialSSE_w_Wwt11 = -transFuncDerivHidden1*inputNode1*(vWt10*error0 + vWt11*error1)\n","    \n","    sumTermH0 = vWt00*error0+vWt01*error1\n","    sumTermH1 = vWt10*error0+vWt11*error1\n","\n","    print(' ')\n","    print('In Print and Trace for Backpropagation: Input to Hidden Weights')\n","    print('  Assuming alpha = 1')\n","    print(' ')\n","    print('  The hidden node activations are:')\n","    print('    Hidden node 0: ', '  %.4f' % hiddenNode0, '  Hidden node 1: ', '  %.4f' % hiddenNode1)\n","    print(' ')\n","    print('  The output node activations are:')\n","    print('    Output node 0: ', '  %.3f' % outputNode0, '   Output node 1: ', '  %.3f' % outputNode1)\n","    print(' ' )\n","    print('  The transfer function derivatives at the hidden nodes are: ')\n","    print('    Deriv-F(0): ', '     %.3f' % transFuncDeriv0, '   Deriv-F(1): ', '     %.3f' % transFuncDeriv1)\n","\n","    print(' ')\n","    print('The computed values for the deltas are: ')\n","    print('                eta  *  error  *   trFncDeriv *   input    * SumTerm for given H')\n","    print('  deltaWWt00 = ',' %.2f' % eta, '* %.4f' % error0, ' * %.4f' % transFuncDerivHidden0, '  * %.4f' % inputNode0, '  * %.4f' % sumTermH0)\n","    print('  deltaWWt01 = ',' %.2f' % eta, '* %.4f' % error1, ' * %.4f' % transFuncDerivHidden1, '  * %.4f' % inputNode0, '  * %.4f' % sumTermH1)                      \n","    print('  deltaWWt10 = ',' %.2f' % eta, '* %.4f' % error0, ' * %.4f' % transFuncDerivHidden0, '  * %.4f' % inputNode1, '  * %.4f' % sumTermH0)\n","    print('  deltaWWt11 = ',' %.2f' % eta, '* %.4f' % error1, ' * %.4f' % transFuncDerivHidden1, '  * %.4f' % inputNode1, '  * %.4f' % sumTermH1)\n","    print(' ')\n","    print('Values for the input-to-hidden connection weights:')\n","    print('           Old:     New:      eta*Delta:')\n","    print('[0,0]:   %.4f' % wWeightArray[0,0], '  %.4f' % newWWeightArray[0,0], '  %.4f' % deltaWWtArray[0,0])\n","    print('[0,1]:   %.4f' % wWeightArray[1,0], '  %.4f' % newWWeightArray[1,0], '  %.4f' % deltaWWtArray[1,0])\n","    print('[1,0]:   %.4f' % wWeightArray[0,1], '  %.4f' % newWWeightArray[0,1], '  %.4f' % deltaWWtArray[0,1])\n","    print('[1,1]:   %.4f' % wWeightArray[1,1], '  %.4f' % newWWeightArray[1,1], '  %.4f' % deltaWWtArray[1,1])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["### Backpropagate weight changes\n","The comments from Dr. Maren below apply to the next four functions\n","<br/>\n","##### \"\n","The first step here applies a backpropagation-based weight change to\n","the hidden-to-output wts v.\n","<br/><br/>\n","Core equation for the first part of backpropagation: <br/><br/>\n","d(SSE)/dv(h,o) = -alpha*Error*F(1-F)*Hidden(h)\n","<br/><br/>\n","where:\n","* SSE = sum of squared errors, and only the error associated with a given output node counts\n","* v(h,o) is the connection weight v between the hidden node h and the output node o\n","* alpha is the scaling term within the transfer function, often set to 1\n","* - (this is included in transfFuncDeriv)\n","* Error = Error(o) or error at the output node o; = Desired(o) - Actual(o)\n","* F = transfer function, here using the sigmoid transfer function\n","* Hidden(h) = the output of hidden node h.\n","<br/><br/>\n","Note that the training rate parameter is assigned in main; Greek letter \"eta\"\n","(looks like n) scales amount of change to connection weight\n","\n","Unpack the errorList and the vWeightArray\n","\n","We will DECREMENT the connection weight v by a small amount proportional\n","to the derivative eqn of the SSE w/r/t the weight v.\n","\n","\n","This means, since there is a minus sign in that derivative, that we will\n","add a small amount. <br/>\n","(Decrementing is -, applied to a (-), which yields a positive.)\n","\n","For the actual derivation of this equation with MATCHING VARIABLE NAMES (easy to understand),\n","please consult: Brain-Based Computing, by AJ Maren (under development, Jan., 2017). Chpt. X.\n","(Meaning: exact chapter is still TBD.)\n","For the latest updates, etc., please visit: www.aliannajmaren.com\n","##### \"\n","A few other inline notes from Dr. Maren:\n","\n","##### \"\n","The equation for the actual dependence of the Summed Squared Error on a\n","given hidden-to-output weight v(h,o) is:\n","<br/><br/>\n","partial(SSE)/partial(v(h,o)) = -alpha*E(o)*F(o)*`[`1-F(o)]*H(h)\n","<br/><br/>\n","The transfer function derivative (transFuncDeriv) returned from\n","computeTransferFnctnDeriv is given as:\n","<br/><br/>\n","transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput)\n","<br/><br/>\n","Therefore, we can write the equation for the partial(SSE)/partial(v(h,o)) as\n","partial(SSE)/partial(v(h,o)) = E(o)*transFuncDeriv*H(h)\n","<br/><br/>\n","The parameter alpha is included in transFuncDeriv\n","##### \"\n","\n","#### Backpropagate weight changes onto the hidden-to-output connection weights"],"metadata":{}},{"source":["# \"Backpropagate weight changes onto the hidden-to-output connection weights\"\n","def BackpropagateOutputToHidden (alpha, eta, errorList, actualAllNodesOutputList\n","                                , vWeightArray):\n","    # \"Unpack the errorList and the vWeightArray\"\n","    error0 = errorList[0]\n","    error1 = errorList[1]\n","    \n","    vWt00 = vWeightArray[0,0]\n","    vWt01 = vWeightArray[1,0]\n","    vWt10 = vWeightArray[0,1]       \n","    vWt11 = vWeightArray[1,1]  \n","    \n","    hiddenNode0 = actualAllNodesOutputList[0]\n","    hiddenNode1 = actualAllNodesOutputList[1]\n","    outputNode0 = actualAllNodesOutputList[2]    \n","    outputNode1 = actualAllNodesOutputList[3]  \n","        \n","    transFuncDeriv0 = computeTransferFnctnDeriv(outputNode0, alpha) \n","    transFuncDeriv1 = computeTransferFnctnDeriv(outputNode1, alpha)\n","    transFuncDerivList = (transFuncDeriv0, transFuncDeriv1) \n","\n","    # \"Note: the parameter 'alpha' in the transfer function shows up in the \n","    # transfer function derivative and so is not included explicitly in \n","    # these equations\"\n","\n","    partialSSE_w_Vwt00 = -error0*transFuncDeriv0*hiddenNode0                                                             \n","    partialSSE_w_Vwt01 = -error1*transFuncDeriv1*hiddenNode0\n","    partialSSE_w_Vwt10 = -error0*transFuncDeriv0*hiddenNode1\n","    partialSSE_w_Vwt11 = -error1*transFuncDeriv1*hiddenNode1                                                                                                                                                                                                                                                                 \n","                                                                                                                                                                                                                                                \n","    deltaVWt00 = -eta*partialSSE_w_Vwt00\n","    deltaVWt01 = -eta*partialSSE_w_Vwt01        \n","    deltaVWt10 = -eta*partialSSE_w_Vwt10\n","    deltaVWt11 = -eta*partialSSE_w_Vwt11 \n","    deltaVWtArray = np.array([[deltaVWt00, deltaVWt10],[deltaVWt01, deltaVWt11]])\n","\n","    vWt00 = vWt00+deltaVWt00\n","    vWt01 = vWt01+deltaVWt01\n","    vWt10 = vWt10+deltaVWt10\n","    vWt11 = vWt11+deltaVWt11 \n","    \n","    newVWeightArray = np.array([[vWt00, vWt10], [vWt01, vWt11]])\n","\n","    PrintAndTraceBackpropagateOutputToHidden (alpha, eta, errorList \n","                                              , actualAllNodesOutputList\n","                                              , transFuncDerivList, deltaVWtArray\n","                                              , vWeightArray, newVWeightArray)    \n","                                                                                                                                            \n","    return (newVWeightArray)  \n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### \"Backpropagate weight changes onto the bias-to-output connection weights\"\n","\n","This is handled similarly to the function above."],"metadata":{}},{"source":["def BackpropagateBiasOutputWeights (alpha, eta, errorList\n","                                    , actualAllNodesOutputList\n","                                    , biasOutputWeightArray):\n","    # \"Unpack the errorList\" \n","    error0 = errorList[0]\n","    error1 = errorList[1]\n","\n","    # \"Unpack the biasOutputWeightArray, we will only be \n","    # modifying the biasOutput terms   \"\n","    biasOutputWt0 = biasOutputWeightArray[0]\n","    biasOutputWt1 = biasOutputWeightArray[1]\n","\n","    # \"Unpack the outputNodes  \"\n","    outputNode0 = actualAllNodesOutputList[2]    \n","    outputNode1 = actualAllNodesOutputList[3] \n","\n","    # \"Compute the transfer function derivatives as a function of the \n","    # output nodes. *Note: As this is being done after the call to the \n","    # backpropagation on the hidden-to-output weights, the transfer \n","    # function derivative computed there could have been used here; \n","    # the calculations are being redone here only to maintain module independence\"              \n","    transFuncDeriv0 = computeTransferFnctnDeriv(outputNode0, alpha) \n","    transFuncDeriv1 = computeTransferFnctnDeriv(outputNode1, alpha) \n","\n","    # \"This is actually an unnecessary step; \n","    # we're not passing the list back. \"\n","    transFuncDerivList = (transFuncDeriv0, transFuncDeriv1) \n","\n","    partialSSE_w_BiasOutput0 = -error0*transFuncDeriv0\n","    partialSSE_w_BiasOutput1 = -error1*transFuncDeriv1    \n","                                                                                                                                                                                                                                                                \n","    deltaBiasOutput0 = -eta*partialSSE_w_BiasOutput0\n","    deltaBiasOutput1 = -eta*partialSSE_w_BiasOutput1\n","\n","    biasOutputWt0 = biasOutputWt0+deltaBiasOutput0\n","    biasOutputWt1 = biasOutputWt1+deltaBiasOutput1 \n","\n","    # \"Note that only the bias weights for the output nodes have been changed.\"\n","    newBiasOutputWeightArray = np.array([biasOutputWt0, biasOutputWt1])\n","\n","    # The print trace function is missing because it hasn't been written\n","    return (newBiasOutputWeightArray)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["#### \"Backpropagate weight changes onto the input-to-hidden connection weights\"\n","\n","This is handled similarly to the function above."],"metadata":{}},{"source":["# \"Backpropagate weight changes onto the input-to-hidden connection weights\"\n","def BackpropagateHiddenToInput (alpha, eta, errorList, actualAllNodesOutputList\n","                                , inputDataList\n","                                , vWeightArray, wWeightArray\n","                                , biasHiddenWeightArray, biasOutputWeightArray):\n","    # Unpack the errorList and the vWeightArray\n","    error0 = errorList[0]\n","    error1 = errorList[1]\n","    \n","    vWt00 = vWeightArray[0,0]\n","    vWt01 = vWeightArray[1,0]\n","    vWt10 = vWeightArray[0,1]       \n","    vWt11 = vWeightArray[1,1]  \n","    \n","    wWt00 = wWeightArray[0,0]\n","    wWt01 = wWeightArray[1,0]\n","    wWt10 = wWeightArray[0,1]       \n","    wWt11 = wWeightArray[1,1] \n","    \n","    inputNode0 = inputDataList[0] \n","    inputNode1 = inputDataList[1]         \n","    hiddenNode0 = actualAllNodesOutputList[0]\n","    hiddenNode1 = actualAllNodesOutputList[1]\n","    outputNode0 = actualAllNodesOutputList[2]    \n","    outputNode1 = actualAllNodesOutputList[3]\n","\n","    # \"For the second step in backpropagation (computing deltas on the \n","    # input-to-hidden weights), we need the transfer function derivative \n","    # applied to the output at the hidden node\"\n","    transFuncDerivHidden0 = computeTransferFnctnDeriv(hiddenNode0, alpha) \n","    transFuncDerivHidden1 = computeTransferFnctnDeriv(hiddenNode1, alpha)\n","    transFuncDerivHiddenList = (transFuncDerivHidden0, transFuncDerivHidden1)\n","\n","    # \"We also need the transfer function derivative applied to \n","    # the output at the output node\"\n","    transFuncDerivOutput0 = computeTransferFnctnDeriv(outputNode0, alpha) \n","    transFuncDerivOutput1 = computeTransferFnctnDeriv(outputNode1, alpha)\n","    transFuncDerivOutputList = (transFuncDerivOutput0, transFuncDerivOutput1) \n","    \n","    errorTimesTransDOutput0 = error0*transFuncDerivOutput0\n","    errorTimesTransDOutput1 = error1*transFuncDerivOutput1\n","\n","    # \"Note: the parameter 'alpha' in the transfer function shows up in \n","    # the transfer function derivative and so is not included explicitly\n","    # in these equations\"\n","    partialSSE_w_Wwt00 = (-transFuncDerivHidden0*inputNode0\n","                            *(vWt00*errorTimesTransDOutput0 \n","                            + vWt01*errorTimesTransDOutput1))                                                             \n","    partialSSE_w_Wwt01 = (-transFuncDerivHidden1*inputNode0\n","                            *(vWt10*errorTimesTransDOutput0 \n","                            + vWt11*errorTimesTransDOutput1))\n","    partialSSE_w_Wwt10 = (-transFuncDerivHidden0*inputNode1\n","                            *(vWt00*errorTimesTransDOutput0 \n","                            + vWt01*errorTimesTransDOutput1))\n","    partialSSE_w_Wwt11 = (-transFuncDerivHidden1*inputNode1\n","                            *(vWt10*errorTimesTransDOutput0 \n","                            + vWt11*errorTimesTransDOutput1))\n","\n","    deltaWWt00 = -eta*partialSSE_w_Wwt00\n","    deltaWWt01 = -eta*partialSSE_w_Wwt01        \n","    deltaWWt10 = -eta*partialSSE_w_Wwt10\n","    deltaWWt11 = -eta*partialSSE_w_Wwt11 \n","    deltaWWtArray = np.array([[deltaWWt00, deltaWWt10]\n","                            ,[deltaWWt01, deltaWWt11]]) \n","\n","    wWt00 = wWt00+deltaWWt00\n","    wWt01 = wWt01+deltaWWt01\n","    wWt10 = wWt10+deltaWWt10\n","    wWt11 = wWt11+deltaWWt11 \n","    \n","    newWWeightArray = np.array([[wWt00, wWt10], [wWt01, wWt11]])\n","\n","    return (newWWeightArray)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["##### \"Backpropagate weight changes onto the bias-to-hidden connection weights\"\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# \"Backpropagate weight changes onto the bias-to-hidden connection weights\"\n","def BackpropagateBiasHiddenWeights (alpha, eta, errorList\n","                                    , actualAllNodesOutputList, vWeightArray\n","                                    , biasHiddenWeightArray\n","                                    , biasOutputWeightArray):\n","   # \"Unpack the errorList and vWeightArray\"\n","    error0 = errorList[0]\n","    error1 = errorList[1]\n","    \n","    vWt00 = vWeightArray[0,0]\n","    vWt01 = vWeightArray[1,0]\n","    vWt10 = vWeightArray[0,1]       \n","    vWt11 = vWeightArray[1,1] \n","\n","    # \"Unpack the biasWeightArray, we will only be modifying the biasOutput \n","    # terms, but need to have all the bias weights for when we redefine \n","    # the biasWeightArray\"\n","    biasHiddenWt0 = biasHiddenWeightArray[0]\n","    biasHiddenWt1 = biasHiddenWeightArray[1]    \n","    biasOutputWt0 = biasOutputWeightArray[0]\n","    biasOutputWt1 = biasOutputWeightArray[1]\n","\n","    # \"Unpack the outputNodes\"  \n","    hiddenNode0= actualAllNodesOutputList[0]\n","    hiddenNode1= actualAllNodesOutputList[1]\n","    outputNode0 = actualAllNodesOutputList[2]    \n","    outputNode1 = actualAllNodesOutputList[3]\n","\n","    # \"Compute the transfer function derivatives as a function of the \n","    # output nodes. *Note: As this is being done after the call to the \n","    # backpropagation on the hidden-to-output weights, the transfer \n","    # function derivative computed there could have been used here; \n","    # the calculations are being redone here only to maintain module independence\"\n","    transFuncDerivOutput0 = computeTransferFnctnDeriv(outputNode0, alpha) \n","    transFuncDerivOutput1 = computeTransferFnctnDeriv(outputNode1, alpha)\n","    transFuncDerivHidden0 = computeTransferFnctnDeriv(hiddenNode0, alpha) \n","    transFuncDerivHidden1 = computeTransferFnctnDeriv(hiddenNode1, alpha) \n","\n","    # \"This list will be used only if we call a print-and-trace debug function.\"\" \n","    transFuncDerivOutputList = (transFuncDerivOutput0, transFuncDerivOutput1)  \n","\n","    errorTimesTransDOutput0 = error0*transFuncDerivOutput0\n","    errorTimesTransDOutput1 = error1*transFuncDerivOutput1\n","    \n","    partialSSE_w_BiasHidden0 = -transFuncDerivHidden0*(errorTimesTransDOutput0*vWt00 + \n","    errorTimesTransDOutput1*vWt01)\n","    partialSSE_w_BiasHidden1 = -transFuncDerivHidden1*(errorTimesTransDOutput0*vWt10 + \n","    errorTimesTransDOutput1*vWt11)  \n","                                                                                                                                                                                                                                                                \n","    deltaBiasHidden0 = -eta*partialSSE_w_BiasHidden0\n","    deltaBiasHidden1 = -eta*partialSSE_w_BiasHidden1\n","\n","    biasHiddenWt0 = biasHiddenWt0+deltaBiasHidden0\n","    biasHiddenWt1 = biasHiddenWt1+deltaBiasHidden1 \n","\n","    newBiasHiddenWeightArray = np.array([biasHiddenWt0, biasHiddenWt1])\n","\n","    return (newBiasHiddenWeightArray) \n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["## The MAIN Module\n","\n","##### \"\n","The MAIN module comprising of calls to:\n","*  Welcome\n","* Obtain neural network size specifications for a three-layer network consisting of:\n","*    - Input layer\n","*    - Hidden layer\n","*    - Output layer (all the sizes are currently hard-coded to two nodes per layer right now)\n","* Initialize connection weight values\n","*    - w: Input-to-Hidden nodes\n","*    - v: Hidden-to-Output nodes\n","* Determine the initial Sum Squared Error (SSE) for each training pair, and also the total SSE\n","##### \""],"metadata":{}},{"source":["# Creating the Main module\n","def main():\n","    # \"Obtain unit array size in terms of array_length (M) and layers (N)\"\"\n","    welcome() # Calls our first procedure to print a message\n","    # Hardcode parameter definitions (replace later with user input)\n","    alpha = 1.0 # \"parameter governing steepness of sigmoid transfer function\"\n","    summedInput = 1\n","    maxNumIterations = 10    # \"temporarily set to 10 for testing\"\n","    eta = 0.5                # \"training rate\"\n","\n","    # For this first MLP network, we're hardcoding the layers\n","    # We'll have an Input, Output, and one Hidden layer\n","\n","    # \"This defines the variable arraySizeList, which is a list.\"\n","    # \"It is initially an empty list. Its purpose is to store the\" \n","    # size of the array.\"\n","\n","    arraySizeList = list() # \"empty list *See Note 1 in section below \"\n","    arraySizeList = obtainNeuralNetworkSizeSpecs ()\n","\n","    # Unpack the list; ascribe the various elements of the list to the \n","    # sizes of different network layers    \n","    inputArrayLength = arraySizeList[0]\n","    hiddenArrayLength = arraySizeList [1]\n","    outputArrayLength = arraySizeList [2]\n","\n","    # See Note 2 below\n","\n","    # Initialize the training list *See Note 3 below\n","    trainingDataList = (0,0,0,0,0)\n","\n","    # Initialize the weight arrays for two sets of weights\n","\n","    # The wWeightArray is for Input-to-Hidden\n","    # The vWeightArray is for Hidden-to-Output\n","    wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n","    vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n","    biasHiddenWeightArraySize = hiddenArrayLength\n","    biasOutputWeightArraySize = outputArrayLength  \n","\n","    # The node-to-node connection weights are stored in a 2-D array    \n","\n","    # Debug parameter for examining results within initializeWeightArray is currently set to False\n","    debugCallInitializeOff = True\n","    debugInitializeOff = True\n","    if not debugCallInitializeOff:\n","        print(' ')\n","        print('Calling initializeWeightArray for input-to-hidden weights')\n","\n","    wWeightArray = initializeWeightArray (wWeightArraySizeList\n","                                          , debugInitializeOff)\n","    \n","    if not debugCallInitializeOff:\n","        print(' ')\n","        print('Calling initializeWeightArray for hidden-to-output weights')\n","\n","    vWeightArray = initializeWeightArray (vWeightArraySizeList\n","                                          , debugInitializeOff)\n","\n","    # The bias weights are stored in a 1-D array         \n","    biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n","    biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) \n","\n","\n","    initialWWeightArray = wWeightArray[:]\n","    initialVWeightArray = vWeightArray[:]\n","    initialBiasHiddenWeightArray = biasHiddenWeightArray[:]   \n","    initialBiasOutputWeightArray = biasOutputWeightArray[:] \n","\n","    print()\n","    print('The initial weights for this neural network are:')\n","    print('       Input-to-Hidden                            Hidden-to-Output')\n","    print('  w(0,0) = %.4f   w(1,0) = %.4f         v(0,0) = %.4f   v(1,0) = %.4f' \n","            % (initialWWeightArray[0,0]\n","            , initialWWeightArray[0,1]\n","            , initialVWeightArray[0,0]\n","            , initialVWeightArray[0,1]))\n","    print('  w(0,1) = %.4f   w(1,1) = %.4f         v(0,1) = %.4f   v(1,1) = %.4f' \n","            % (initialWWeightArray[1,0]\n","            , initialWWeightArray[1,1]\n","            , initialVWeightArray[1,0]\n","            , initialVWeightArray[1,1]))\n","    print(' ')\n","    print('       Bias at Hidden Layer                          Bias at Output Layer')\n","    print('       b(hidden,0) = %.4f                           b(output,0) = %.4f' \n","            % (biasHiddenWeightArray[0], biasOutputWeightArray[0] ))                  \n","    print('       b(hidden,1) = %.4f                           b(output,1) = %.4f' \n","            % (biasHiddenWeightArray[1], biasOutputWeightArray[1] ))  \n","  \n","    epsilon = 0.2\n","    iteration = 0\n","    SSE_InitialTotal = 0.0\n","\n","   # \"Next step - Get an initial value for the \n","   # Total Summed Squared Error (Total_SSE)\"\n","\n","   # \"Initialize an array of SSE values\" * See Note 4 below\n","    SSE_InitialArray = [0,0,0,0,0]\n","    \n","    # \"Before starting the training run, compute the initial SSE Total \n","    #   (sum across SSEs for each training data set)\" \n","    debugSSE_InitialComputationOff = True\n","\n","    SSE_InitialArray = computeSSE_Values (alpha, SSE_InitialArray\n","                                          , wWeightArray, vWeightArray\n","                                          , biasHiddenWeightArray\n","                                          , biasOutputWeightArray\n","                                          , debugSSE_InitialComputationOff)\n","\n","    # \"Start the SSE_Array at the same values as the Initial SSE Array\"\n","    SSE_Array = SSE_InitialArray[:] \n","    SSE_InitialTotal = SSE_Array[4] \n","\n","    # \"Set a local debug print parameter\"(this step optional)\n","    debugSSE_InitialComputationReportOff = True    \n","\n","    if not debugSSE_InitialComputationReportOff:\n","        print(' ')\n","        print('In main, SSE computations completed, Total of all SSEs = %.4f' \n","                % SSE_Array[4])\n","        print('  For input nodes (0,0), SSE_Array[0] = %.4f' % SSE_Array[0])\n","        print('  For input nodes (0,1), SSE_Array[1] = %.4f' % SSE_Array[1])\n","        print('  For input nodes (1,0), SSE_Array[2] = %.4f' % SSE_Array[2])\n","        print('  For input nodes (1,1), SSE_Array[3] = %.4f' % SSE_Array[3])\n","\n","# Running the model:\n","# \"Next step - Obtain a single set of input values for the X-OR problem; \n","# two integers - can be 0 or 1\"\n","\n","    while iteration < maxNumIterations:\n","         # \"Randomly select one of four training sets; the inputs will be \n","         # randomly assigned to 0 or 1\"\n","        trainingDataList = obtainRandomXORTrainingValues () \n","        input0 = trainingDataList[0]\n","        input1 = trainingDataList[1] \n","        desiredOutput0 = trainingDataList[2]\n","        desiredOutput1 = trainingDataList[3]\n","        setNumber = trainingDataList[4]       \n","        print(' ')\n","        print('Randomly selecting XOR inputs for XOR, identifying desired outputs for this training pass:')\n","        print('          Input0 = ', input0,         '            Input1 = ', input1)   \n","        print(' Desired Output0 = ', desiredOutput0, '   Desired Output1 = ', desiredOutput1)    \n","        print(' ')\n","\n","        # \"Compute a single feed-forward pass\"\n","\n","        # Initialize the error list\n","        errorList = (0,0)\n","    \n","        # \"Initialize the actualOutput list\"\n","        actualAllNodesOutputList = (0,0,0,0)     \n","\n","        # \"Create the inputData list\"      \n","        inputDataList = (input0, input1)         \n","    \n","        # \"Compute a single feed-forward pass and obtain the Actual Outputs\"\n","        debugComputeSingleFeedforwardPassOff = True\n","        actualAllNodesOutputList = ComputeSingleFeedforwardPass (alpha\n","                                        , inputDataList\n","                                        , wWeightArray\n","                                        , vWeightArray\n","                                        , biasHiddenWeightArray\n","                                        , biasOutputWeightArray\n","                                        ,debugComputeSingleFeedforwardPassOff)\n","\n","        # \"Assign the hidden and output values to specific different variables\"\n","        actualHiddenOutput0 = actualAllNodesOutputList [0] \n","        actualHiddenOutput1 = actualAllNodesOutputList [1] \n","        actualOutput0 = actualAllNodesOutputList [2]\n","        actualOutput1 = actualAllNodesOutputList [3] \n","    \n","        # \"Determine the error between actual and desired outputs\"\n","\n","        error0 = desiredOutput0 - actualOutput0\n","        error1 = desiredOutput1 - actualOutput1\n","        errorList = (error0, error1)\n","    \n","        # \"Compute the Summed Squared Error, or SSE\"\n","        SSEInitial = error0**2 + error1**2\n","        \n","                            \n","        debugMainComputeForwardPassOutputsOff = True\n","\n","        # \"Debug print the actual outputs from the two output neurons\"\n","        if not debugMainComputeForwardPassOutputsOff:\n","            print(' ')\n","            print('In main; have just completed a feedfoward pass with training set inputs'\n","                                                                , input0, input1)\n","            print('  The activations (actual outputs) for the two hidden neurons are:')\n","            print('    actualHiddenOutput0 = %.4f' % actualHiddenOutput0)\n","            print('    actualHiddenOutput1 = %.4f' % actualHiddenOutput1)   \n","            print('  The activations (actual outputs) for the two output neurons are:')\n","            print('    actualOutput0 = %.4f' % actualOutput0)\n","            print('    actualOutput1 = %.4f' % actualOutput1)\n","            print('  Initial SSE (before backpropagation) = %.6f' % SSEInitial)\n","            print('  Corresponding SSE (from initial SSE determination) = %.6f' \n","                                                        % SSE_Array[setNumber])\n","\n","        # \"Perform first part of the backpropagation of weight changes\"\n","        newVWeightArray = BackpropagateOutputToHidden (alpha, eta, errorList\n","                                                      , actualAllNodesOutputList\n","                                                      , vWeightArray)\n","\n","        newBiasOutputWeightArray = BackpropagateBiasOutputWeights (alpha, eta\n","                                                    , errorList\n","                                                    , actualAllNodesOutputList\n","                                                    , biasOutputWeightArray)\n","        newBiasOutputWeight0 = newBiasOutputWeightArray[0]\n","        newBiasOutputWeight1 = newBiasOutputWeightArray[1]\n","        \n","        newWWeightArray = BackpropagateHiddenToInput (alpha, eta, errorList\n","                                                    , actualAllNodesOutputList\n","                                                    , inputDataList, vWeightArray\n","                                                    , wWeightArray\n","                                                    , biasHiddenWeightArray\n","                                                    , biasOutputWeightArray)\n","\n","        newBiasHiddenWeightArray = BackpropagateBiasHiddenWeights (alpha, eta\n","                                                , errorList\n","                                                , actualAllNodesOutputList\n","                                                , vWeightArray\n","                                                , biasHiddenWeightArray\n","                                                , biasOutputWeightArray)\n","        newBiasHiddenWeight0 = newBiasHiddenWeightArray[0]\n","        newBiasHiddenWeight1 = newBiasHiddenWeightArray[1]        \n","        \n","        newBiasWeightArray = [[newBiasOutputWeight0, newBiasOutputWeight1]\n","                            , [newBiasHiddenWeight0, newBiasHiddenWeight1]] \n","\n","        # \"Debug prints on the weight arrays\"\n","        debugWeightArrayOff = False\n","\n","        if not debugWeightArrayOff:\n","            print(' ')\n","            print('    The weights before backpropagation are:')\n","            print('         Input-to-Hidden                           Hidden-to-Output')\n","            print('    w(0,0) = %.3f   w(1,0) = %.3f         v(0,0) = %.3f   v(1,0) = %.3f' \n","                        % (wWeightArray[0,0], wWeightArray[0,1]\n","                        , vWeightArray[0,0], vWeightArray[0,1]))\n","            print('    w(0,1) = %.3f   w(1,1) = %.3f         v(0,1) = %.3f   v(1,1) = %.3f' \n","                        % (wWeightArray[1,0], wWeightArray[1,1]\n","                        , vWeightArray[1,0], vWeightArray[1,1])) \n","            print(' ')\n","            print('    The weights after backpropagation are:')\n","            print('         Input-to-Hidden                           Hidden-to-Output')\n","            print('    w(0,0) = %.3f   w(1,0) = %.3f         v(0,0) = %.3f   v(1,0) = %.3f' \n","                        % (newWWeightArray[0,0], newWWeightArray[0,1]\n","                        , newVWeightArray[0,0], newVWeightArray[0,1]))\n","            print('    w(0,1) = %.3f   w(1,1) = %.3f         v(0,1) = %.3f   v(1,1) = %.3f' \n","                        % (newWWeightArray[1,0], newWWeightArray[1,1]\n","                        , newVWeightArray[1,0], newVWeightArray[1,1]))\n","\n","        # \"Assign the old hidden-to-output weight array to be the same as \n","        # what was returned from the BP weight update\"\n","        vWeightArray = newVWeightArray[:]\n","    \n","        # \"Assign the old input-to-hidden weight array to be the same as \n","        # what was returned from the BP weight update\"\n","        wWeightArray = newWWeightArray[:]\n","    \n","        # \"Run the computeSingleFeedforwardPass again, to compare the \n","        # results after just adjusting the hidden-to-output weights\"\n","        newAllNodesOutputList = ComputeSingleFeedforwardPass (alpha, inputDataList, wWeightArray, vWeightArray,\n","        biasHiddenWeightArray, biasOutputWeightArray, debugComputeSingleFeedforwardPassOff)         \n","        newOutput0 = newAllNodesOutputList [2]\n","        newOutput1 = newAllNodesOutputList [3] \n","\n","        # \"Determine the new error between actual and desired outputs\"\n","        newError0 = desiredOutput0 - newOutput0\n","        newError1 = desiredOutput1 - newOutput1\n","        newErrorList = (newError0, newError1)\n","\n","        # \"Compute the new Summed Squared Error, or SSE\"\n","        SSE0 = newError0**2\n","        SSE1 = newError1**2\n","        newSSE = SSE0 + SSE1\n","\n","        # \"Print the Summed Squared Error\"\n","\n","        # \"Debug print the actual outputs from the two output neurons\"\n","        if not debugMainComputeForwardPassOutputsOff:\n","            print(' ')\n","            print('In main; have just completed a single step of backpropagation with inputs'\n","                                                               , input0, input1)\n","            print('    The new SSE (after backpropagation) = %.6f' % newSSE)\n","            print('    Error(0) = %.4f,   Error(1) = %.4f' %(NewError0, NewError1))\n","            print('    SSE(0) =   %.4f,   SSE(1) =   %.4f' % (SSE0, SSE1))\n","            deltaSSE = SSEInitial - newSSE\n","            print('  The difference in initial and the resulting SSEs is: %.4f' \n","                                                                    % deltaSSE) \n","            \n","            if deltaSSE >0:\n","                print(' ')\n","                print('   The training has resulted in improving the total SSEs')\n","\n","       # \"Assign the SSE to the SSE for the appropriate training set\"\n","        SSE_Array[setNumber] = newSSE\n","\n","        # \"Obtain the previous SSE Total from the SSE array\"\n","        previousSSE_Total = SSE_Array[4]\n","        print(' ' )\n","        print('  The previous SSE Total was %.4f' % previousSSE_Total)\n","\n","        # \"Compute the new sum of SSEs (across all the different training sets)\n","        #   ... this will be different because we've changed one of the SSE's\"\n","        newSSE_Total = SSE_Array[0] + SSE_Array[1] +SSE_Array[2] + SSE_Array[3]\n","\n","        print('  The new SSE Total was %.4f' % newSSE_Total)\n","        print('    For node 0: Desired Output = ',desiredOutput0,  ' New Output = %.4f' \n","                                                                    % newOutput0) \n","        print('    For node 1: Desired Output = ',desiredOutput1,  ' New Output = %.4f' \n","                                                                    % newOutput1)  \n","        print('    Error(0) = %.4f,   Error(1) = %.4f' %(newError0, newError1))\n","        print('    SSE0(0) =   %.4f,   SSE(1) =   %.4f' % (SSE0, SSE1) )\n","\n","        # \"Assign the new SSE to the final place in the SSE array\"\n","        SSE_Array[4] = newSSE_Total\n","        deltaSSE = previousSSE_Total - newSSE_Total\n","\n","        print('  Delta in the SSEs is %.4f' % deltaSSE)\n","        if deltaSSE > 0:\n","            print('SSE improvement')\n","        else: print('NO improvement')\n","\n","       # \"Assign the new errors to the error list\"             \n","        errorList = newErrorList[:]\n","        \n","        \n","        print(' ')\n","        print('Iteration number ', iteration)\n","        iteration = iteration + 1\n","\n","        if newSSE_Total < epsilon:\n","\n","            \n","            break\n","    print('Out of while loop')\n","\n","    debugEndingSSEComparisonOff = False\n","\n","    if not debugEndingSSEComparisonOff:\n","        SSE_Array[4] = newSSE_Total\n","        deltaSSE = previousSSE_Total - newSSE_Total\n","        print('  Initial Total SSE = %.4f'  % SSE_InitialTotal)\n","        print('  Final Total SSE = %.4f'  % newSSE_Total)\n","\n","        finalDeltaSSE = SSE_InitialTotal - newSSE_Total\n","\n","        print('  Delta in the SSEs is %.4f' % finalDeltaSSE )\n","        if finalDeltaSSE > 0:\n","            print('SSE total improvement')\n","        else: print('NO improvement in total SSE')\n","\n","# Conclude specification of the MAIN procedure\n","if __name__ == \"__main__\": main()  \n","\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["Note 1: \"# Notice that I'm using the same variable name, 'arraySizeList'\n","both here in main and in the called procedure, 'obtainNeuralNetworkSizeSpecs.'\n","I don't have to use the same name; the procedure returns a list and I'm\n","assigning it HERE to the list named arraySizeList in THIS 'main'\n","procedure. I could use different names. I'm keeping the same name so\n","that it is easier for us to connect what happens in the called procedure\n","'obtainNeuralNetworkSizeSpecs' with this procedure, 'main.' \"\n","<br/><br/>\n","Note 2: # \"I have all sorts of debug statements left in this, so you can\n","trace the code moving into and out of various procedures and functions.\"\n","Try these:\n","<br/><br/>\n","print('Flow-of-control trace: Back in main')<br/>\n","print('I = number of nodes in input layer is', inputArrayLength)<br/>\n","print('H = number of nodes in hidden layer is', hiddenArrayLength)<br/>\n","print('O = number of nodes in output layer is', outputArrayLength)<br/>\n","<br/><br/>\n","Note 3: \"The training list has, in order, the two input nodes, the two\n","output nodes (this is a two-output version of the X-OR problem), and\n","the data set number (0..3), meaning that each data set is numbered.<br/>\n","This helps in going through the entire data set once the initial weights\n","are established to get a total sum (across all data sets) of the\n","Summed Squared Error, or SSE.\"\n","<br/<br/>\n","Note 4: \"The first four SSE values are the SSE's for specific\n","input/output pairs-- the fifth is the sum of all the SSE's.\"\n","<br/><br/>\n","Note 5:"],"metadata":{}},{"cell_type":"markdown","source":["#### Additional print options for MAIN module\n","If desired, all or some of the below code can be used after the final\n","print statement in the Main procedure to give stats on weights and SSEs\n","before and during training.\n","<br/><br/>\n","I left them out above to increase readability. They have not been\n","edited for line length or readability in any other way."],"metadata":{}},{"source":["#    print ' '\n","#    print 'The initial weights for this neural network are:'\n","#    print '     Input-to-Hidden                       Hidden-to-Output'\n","#    print 'w(0,0) = %.3f   w(0,1) = %.3f         v(0,0) = %.3f   v(0,1) = %.3f' % (initialWWeightArray[0,0], \n","#    initialWWeightArray[0,1], initialVWeightArray[0,0], initialVWeightArray[0,1])\n","#    print 'w(1,0) = %.3f   w(1,1) = %.3f         v(1,0) = %.3f   v(1,1) = %.3f' % (initialWWeightArray[1,0], \n","#    initialWWeightArray[1,1], initialVWeightArray[1,0], initialVWeightArray[1,1])        \n","\n","                                                                                    \n","#    print ' '\n","#    print 'The final weights for this neural network are:'\n","#    print '     Input-to-Hidden                       Hidden-to-Output'\n","#    print 'w(0,0) = %.3f   w(0,1) = %.3f         v(0,0) = %.3f   v(0,1) = %.3f' % (wWeightArray[0,0], \n","#    wWeightArray[0,1], vWeightArray[0,0], vWeightArray[0,1])\n","#    print 'w(1,0) = %.3f   w(1,1) = %.3f         v(1,0) = %.3f   v(1,1) = %.3f' % (wWeightArray[1,0], \n","#    wWeightArray[1,1], vWeightArray[1,0], vWeightArray[1,1])        \n","                                                                                    \n","   \n","# Print the SSE's at the beginning of training\n","#    print ' '\n","#    print 'The SSE values at the beginning of training were: '\n","#    print '  SSE_Initial[0] = %.4f' % SSE_InitialArray[0]\n","#    print '  SSE_Initial[1] = %.4f' % SSE_InitialArray[1]\n","#    print '  SSE_Initial[2] = %.4f' % SSE_InitialArray[2]\n","#    print '  SSE_Initial[3] = %.4f' % SSE_InitialArray[3]    \n","#    print ' '\n","#    print 'The total of the SSE values at the beginning of training is %.4f' % SSE_InitialTotal \n","\n","\n","# Print the SSE's at the end of training\n","#    print ' '\n","#    print 'The SSE values at the end of training were: '\n","#    print '  SSE[0] = %.4f' % SSE_Array[0]\n","#    print '  SSE[1] = %.4f' % SSE_Array[1]\n","#    print '  SSE[2] = %.4f' % SSE_Array[2]\n","#    print '  SSE[3] = %.4f' % SSE_Array[3]    \n","#    print ' '\n","#    print 'The total of the SSE values at the end of training is %.4f' % SSE_Array[4]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n","# Print comparison of previous and new outputs             \n","#    print ' ' \n","#    print 'Values for the new outputs compared with previous, given only a partial backpropagation training:'\n","#    print '     Old:', '   ', 'New:', '   ', 'nu*Delta:'\n","#    print 'Output 0:  Desired = ', desiredOutput0, 'Old actual =  %.4f' % actualOutput0, 'Newactual  %.4f' % newOutput0\n","#    print 'Output 1:  Desired = ', desiredOutput1, 'Old actual =  %.4f' % actualOutput1, 'Newactual  %.4f' % newOutput1"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}