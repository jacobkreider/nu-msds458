{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook created by Jacob Kreider*\n",
    "\n",
    "Notes from 'Deep Learning with Python' by Francois Chollet, Manning Press\n",
    "<br/>\n",
    "*Anything in quotes in the markdown sections is a direct quote from the book*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - 3.3.4\n",
    "Notes to be transcribed from handwritten later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 The IMDB Dataset\n",
    "Working with IMDB data to classify reviews as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading/loading the built-in imdb data\n",
    "from keras.datasets import imdb\n",
    "\n",
    "#Setting up train and test data\n",
    "(trainData, trainLabels), (testData, testLabels) = imdb.load_data(\n",
    "    num_words = 10000 #Only keep top 10K words\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoding one of the reviews back to English, just to see how it's done\n",
    "wordIndex = imdb.get_word_index()\n",
    "reverseWordIndex = dict(\n",
    "    [(value, key) for (key, value) in wordIndex.items()])\n",
    "decodedReview = ' '.join(\n",
    "    [reverseWordIndex.get(i - 3, '?') for i in trainData[0]])\n",
    "print(decodedReview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Preparing the data\n",
    "\"You can't feed a list of integers into a neural network. You have to turn them\n",
    "into into tensors. There are two ways to do that:<br/><br/>\n",
    "1. Pad your lists so they all have the same length, turn them into n integer\n",
    "tensor of shape (samples, word_indices), and then use as the first layer in\n",
    "your network-- a layer capable of handling such integer tensors (the 'embedding'\n",
    "layer, covered later in the book).<br/><br/>\n",
    "2. One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean,\n",
    "for instance, turning the sequence [3, 5] into a 10K-dimensional vector\n",
    "that would all be zeroes except for indices 3 and 5, which would be ones. Then you\n",
    "could use as the first layer in your network a 'Dense' layer, capable of handling\n",
    "floating-point vector data\n",
    "\n",
    "Let's go with the latter solution to vectorize the data, which you;ll do manually for\n",
    "maximum clarity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Create an all-zero matrix of shape(len(sequences), dimension)\n",
    "def vectorizeSequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        #Set specific indices of results[i] to 1s\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "xTrain = vectorizeSequences(trainData) #Vectorized training data\n",
    "xTest = vectorizeSequences(testData) #Vectorized test data\n",
    "\n",
    "#Vectorize the labels, as well\n",
    "yTrain = np.asarray(trainLabels).astype('float32')\n",
    "yTest = np.asarray(testLabels).astype('float32')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Building Your Network\n",
    "\"The input data are vectors, and the labels are scalars.\"\n",
    "This type of data works well with a simple stack of fully connected ('Dense')\n",
    "layers with 'relu' activations : Dense(16, activation = 'relu')<br/>\n",
    "The above line passes 16 to the Dense layer because that's the\n",
    "number of 'hidden units' in the layer. <br/><br/>\n",
    "Hidden Units = dimension in the representation space\n",
    "of the layer. A way of thinking about hidden units is that they\n",
    "represent \"how much freedom you're allowing the representation to\n",
    "have when learning internal representations.\" As hidden units (dimensional\n",
    "representation space) increases, your model can handle higher-complexity\n",
    "problems, but computational complexity goes up, as does the potential\n",
    "for overfitting.\n",
    "### Two Key Architecture Decisions - How many layers to use and\n",
    "### how many hidden units per layer\n",
    "(We'll cover how to do this in the next chapter. For now, he chooses for us)<br/><br/>\n",
    "For this chapter, we'll use this architecture:\n",
    "* Two intermediate layers with 16 hidden units each\n",
    "* A third, output layer that will return the scalar prediction\n",
    "Relu activation will be used on the intermediate layers, and we'll use signmoid\n",
    "activation on the output layer so that we get probability scores<br/><br/>\n",
    "*Note: Relu (rectified linear unit) zeroes out negative numbers*\n",
    "# The Model Definition in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are activation functions and why are they necessary?\n",
    "Activation functions like 'relu' provide the ability to deal with non-linearity.\n",
    "Without them, the 'Dense' layer would only consist of linear operations-- dot product\n",
    "and addition (output = dot(W, input) + b)<br/><br/>\n",
    "If this were the case, we could only handle linear transformations: \"The hypthesis\n",
    "space of the layer would be the set of all possible linear transformations of the\n",
    "input data into a 16-dimensional space.\" Therefore, adding extra layers would not add\n",
    "any extra benefit, as each successive stack would still just be implementing linear\n",
    "operations.<br/><br/>\n",
    "relu is the most common activation function, but there are many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a loss function and an optimizer\n",
    "In this problem, we are performing a binary classification with probability\n",
    "as the output, so we'l be using *binary_crossentropy* as our loss function.\n",
    "(We could also use something like *mean_squared_error*, but binary_crossentropy\n",
    "is a better choice when we're dealing with output probabilities.)<br/><br/>\n",
    "#### *Crossentropy* measures the distance between probability distributions. In this example, it measures the distance between the actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We configure the model in this step\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# This is not the only option. We're passing the optimizer, loss, and metrics as strings\n",
    "# because they are packaged in keras. If we wanted to either configure the parameters of \n",
    "# the optimizer, we could pass it as a class instance, seen here:\n",
    "\n",
    "# from keras import optimizers\n",
    "# model.compile(optimizer = optimizers.RMSprop(lr = 0.001),)\n",
    "# <br/>\n",
    "\n",
    "# If we wanted to pass custom loss functions or metrics, we could create them as a \n",
    "# function, then pass them as the loss or metric arguments:<br/>\n",
    "\n",
    "# loss = losses.binary_crossentropy,\n",
    "# metrics = [metrics.binary_accuracy])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Validating your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll split off a validation set from the training data\n",
    "xVal = xTrain[:10000]\n",
    "partialXtrain = xTrain[10000:]\n",
    "yVal = yTrain[:10000]\n",
    "partialYtrain = yTrain[10000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll train the model for 20 *epochs* (which just means we'll iterate\n",
    "# over the xTrain and yTrain tensors 20 times). We'll use *mini-batches* of 512\n",
    "# samples. Loss and accuracy will be monitored on our validation set.\n",
    "# This is achieved by passing xVal and yVal to the 'validation_data' argument\n",
    "\n",
    "history = model.fit(partialXtrain,\n",
    "                    partialYtrain,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (xVal, yVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The *model.fit* call above returns a history object. This contains a *member history*\n",
    "# which is a dict containing data about every event in the training of the model.<br/>\n",
    "\n",
    "# Examine the history:\n",
    "historyDict = history.history\n",
    "historyDict.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the learning history\n",
    "There are four entries in our example-- one per metric in the training and validation.\n",
    "We can use Matplotlib to plot the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "historyDict = history.history\n",
    "lossValues = historyDict['loss']\n",
    "valLossValues = historyDict['val_loss']\n",
    "\n",
    "epochs = range(1, len(lossValues) + 1)\n",
    "\n",
    "plt.plot(epochs, lossValues, 'bo', label = 'Training Loss')\n",
    "plt.plot(epochs, valLossValues, 'b', label = 'Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation accuracy\n",
    "plt.clf()\n",
    "accValues = historyDict['acc']\n",
    "valAccValues = historyDict['val_acc']\n",
    "\n",
    "plt.plot(epochs, accValues, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, valAccValues, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above charts, we see the telltale signs of *overfitting*. While accuracy increased\n",
    "with each successive epoch on the training set, the validation accuracy peaked at about 4\n",
    "or 5 epochs into the training. After just the second epoch, we were learning\n",
    "representations that only really apply to the training data.<br/>\n",
    "Next, we'll retain the model from scratch, but use only four epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(xTrain, yTrain, epochs = 4, batch_size = 512)\n",
    "results = model.evaluate(xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model, we achieved slightly higher accuracy than our first model (88& vs 85%)\n",
    "using a simpler, naive method that was far less computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 Using a trained network to generate predictions on new data\n",
    "Now that the model is trained, we can call *predict* to have it tell us the\n",
    "likelihood that a review is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(xTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.6 Further Experiments\n",
    "The following experiments will help convince you that the architecture choices\n",
    "you’ve made are all fairly reasonable, although there’s still room for improvement:\n",
    "* You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.\n",
    "* Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.\n",
    "* Try using the mse loss function instead of binary_crossentropy.\n",
    "* Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 1 hidden layer:\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(partialXtrain,\n",
    "                    partialYtrain,\n",
    "                    epochs=4,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(xVal, yVal))\n",
    "\n",
    "historyDict = history.history\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.clf()\n",
    "accValues = historyDict['acc']\n",
    "valAccValues = historyDict['val_acc']\n",
    "\n",
    "epochs = range(1, len(accValues) + 1)\n",
    "\n",
    "plt.plot(epochs, accValues, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, valAccValues, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate test data\n",
    "model.evaluate(xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 3 hidden layers:\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(partialXtrain,\n",
    "                    partialYtrain,\n",
    "                    epochs=4,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(xVal, yVal))\n",
    "\n",
    "historyDict = history.history\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.clf()\n",
    "accValues = historyDict['acc']\n",
    "valAccValues = historyDict['val_acc']\n",
    "\n",
    "epochs = range(1, len(accValues) + 1)\n",
    "\n",
    "plt.plot(epochs, accValues, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, valAccValues, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate test data\n",
    "model.evaluate(xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing and increasing layers caused slight changes to loss and accuracy.\n",
    "Interestingly, the single layer model performed (marginally) better than either\n",
    "the 2 or 3 layer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MSE instead of binary crossentropy\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'mse',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(partialXtrain,\n",
    "                    partialYtrain,\n",
    "                    epochs=4,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(xVal, yVal))\n",
    "\n",
    "historyDict = history.history\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.clf()\n",
    "accValues = historyDict['acc']\n",
    "valAccValues = historyDict['val_acc']\n",
    "\n",
    "epochs = range(1, len(accValues) + 1)\n",
    "\n",
    "plt.plot(epochs, accValues, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, valAccValues, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy with MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate test data\n",
    "model.evaluate(xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With MSE as the loss function, loss dropped significantly, but I'm not sure if that's\n",
    "because MSE and crossentropy produce values on a different scale or not. Test set accuracy\n",
    "was slightly lower than crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the tanh activation\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'tanh', input_shape = (10000, )))\n",
    "model.add(layers.Dense(16, activation = 'tanh'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(partialXtrain,\n",
    "                    partialYtrain,\n",
    "                    epochs=4,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(xVal, yVal))\n",
    "\n",
    "historyDict = history.history\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.clf()\n",
    "accValues = historyDict['acc']\n",
    "valAccValues = historyDict['val_acc']\n",
    "\n",
    "epochs = range(1, len(accValues) + 1)\n",
    "\n",
    "plt.plot(epochs, accValues, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, valAccValues, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy with MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate test data\n",
    "model.evaluate(xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh activation returned similar results to binary crossentropy with MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try using layers with more hidden units or fewer hidden units\n",
    "\n",
    "# 8 hiddent units\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation = 'tanh', input_shape = (10000, )))\n",
    "model.add(layers.Dense(8, activation = 'tanh'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(partialXtrain,\n",
    "                    partialYtrain,\n",
    "                    epochs=4,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(xVal, yVal))\n",
    "\n",
    "historyDict = history.history\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.clf()\n",
    "accValues = historyDict['acc']\n",
    "valAccValues = historyDict['val_acc']\n",
    "\n",
    "epochs = range(1, len(accValues) + 1)\n",
    "\n",
    "plt.plot(epochs, accValues, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, valAccValues, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy with MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate test data\n",
    "model.evaluate(xTest, yTest)\n",
    "\n",
    "# 32 hidden units\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation = 'tanh', input_shape = (10000, )))\n",
    "model.add(layers.Dense(32, activation = 'tanh'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(partialXtrain,\n",
    "                    partialYtrain,\n",
    "                    epochs=4,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(xVal, yVal))\n",
    "\n",
    "historyDict = history.history\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.clf()\n",
    "accValues = historyDict['acc']\n",
    "valAccValues = historyDict['val_acc']\n",
    "\n",
    "epochs = range(1, len(accValues) + 1)\n",
    "\n",
    "plt.plot(epochs, accValues, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, valAccValues, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy with MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate test data\n",
    "model.evaluate(xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 hidden units produced a slight increase in accuracy, 32 a slight drop.\n",
    "### Note that none of these changes in accuracy tell us any base truths about these changes. They all depend on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Classifying newsires: a multiclass classification example\n",
    "\"In this section, you’ll build a network to classify Reuters newswires into 46 mutually exclusive topics. Because you have\n",
    "many classes, this problem is an instance of multiclass classification; and because each data point should be classified\n",
    "into only one category, the problem is more specifically an instance of single-label, multiclass classification.\n",
    "If each data point could belong to multiple categories (in this case, topics), you’d be facing a multilabel, multiclass\n",
    "classification problem.\"\n",
    "#### *Single-label, multiclass classification*: each data point gets thrown into a single category, of which there are many\n",
    "#### *Multilabel, multiclass classification* : each data point can belong to multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 The Reuters Dataset\n",
    "The Reuters datset contains short newswires and their topics, of which there are 46. Each topic has at least 10 examples in\n",
    "the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from keras.datasets import reuters\n",
    "\n",
    "# Create train and test data\n",
    "(trainData, trainLabels), (testData, testLabels) = reuters.load_data(\n",
    "    num_words = 10000) #restricts the data to the 10K most frequently used words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of examples in the train and test sets\n",
    "print(len(trainData))\n",
    "len(testData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our earlier example, each example in the training set is a list of integers\n",
    "that map back to an index of words.\n",
    "The label associated with each example is an integer between 0-45 that maps back to\n",
    "an index of topics.\n",
    "### 3.5.2 Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll vectorize the data using the same code we used in the last exercise\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorizeSequences(sequences, dimension = 10000):\n",
    "    results =np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "xTrain = vectorizeSequences(trainData)\n",
    "xTest = vectorizeSequences(testData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple options for vectorizing the labels: cast the list as an integer tensor,\n",
    "or use one-hot encoding (which is discussed further in chapter 6)\n",
    "In this case, one-hot encoding is implemented the same way that the vectorization was above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(labels, dimension = 46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1\n",
    "    return results\n",
    "\n",
    "oneHotTrainLabels = oneHot(trainLabels)\n",
    "oneHotTestLabels = oneHot(testLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras can do this for us sing to_categorical\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "oneHotTrainLabels = to_categorical(trainLabels)\n",
    "oneHotTestLabels = to_categorical(testLabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Building Your Network\n",
    "While this problem is similar to our movie review classifications, the dimensionality\n",
    "is much higher-- we've gone from two classification groups to 46.\n",
    "<br/>\n",
    "A 16-dimensional space (hidden units) likely won't work here as it did in the last problem.\n",
    "As information passes through stacks of Dense layers, the layer might drop some of that\n",
    "information. When it does, it can't be recovered by deeper layers. This can create an\n",
    "\"information bottleneck\", where relevant information for the output is permanently dropped.\n",
    "To avoid that, we'll increase the dimensionality of our hidden layers by increasing the\n",
    "hidden units to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes above the above architecture:\n",
    "* The output layer has dimensionality of 46 to match the topic list\n",
    "* The *softmax* activation in the final layer returns a probability distribution across the 46 classes.\n",
    "* The softmax probability distribution will sum to 1 and give the likelihood that the inout belongs to each class.\n",
    "The best loss function in this case is *categorical_crossentropy*. This measures tje distance between two probability\n",
    "distributions-- by minimizing this, we train the network to get as close to the true labels as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Validating Your Approach\n",
    "We;ll create a validation set and train the model for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the validation data\n",
    "xVal = xTrain[:1000]\n",
    "partialXtrain = xTrain[1000:]\n",
    "\n",
    "yVal = oneHotTrainLabels[:1000]\n",
    "partialYtrain = oneHotTrainLabels[1000:]\n",
    "\n",
    "# Train the model\n",
    "\n",
    "history = model.fit(partialXtrain,\n",
    "                    partialYtrain,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(xVal, yVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loss and accuracy curves for the model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "valLoss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = \"Training Loss\")\n",
    "plt.plot(epochs, valLoss, 'b', label = \"Validation Loss\")\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Accuracy Curve:\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "acc = history.history['acc']\n",
    "valAcc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, valAcc, 'b', label = \"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 9 epochs, the model begins to overfit (there is a slight drop in accuracy at that\n",
    "point before it increases throughout the remaining iterations)\n",
    "<br/><br/>\n",
    "We'll rebuild the model from scratch using only 9 epochs and evaluate it against our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model from scratch\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(partialXtrain,\n",
    "          partialYtrain,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(xVal, yVal))\n",
    "\n",
    "results = model.evaluate(xTest, oneHotTestLabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model results of 77% accuracy far outperform the random baseline of ~19%\n",
    "### 3.5.5 Generating predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate prediction on the test data\n",
    "predictions = model.predict(xTest)\n",
    "\n",
    "# Each entry should be a vector with the same lenght as the number of topics (46)\n",
    "predictions[0].shape\n",
    "\n",
    "# and the coefficients of each of those vectors should sum to one\n",
    "np.sum(predictions[0])\n",
    "\n",
    "#Whatever class in each vector has the highest value is the predicted class\n",
    "np.argmax(predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.6 A different way to handle labels and loss\n",
    "We could have cast the labels as an integer tensor instead of one hot encoding them. To do\n",
    "this, you just call yTrain = np.array(trainLabels).\n",
    "<br/><br/>\n",
    "Not much would change by doing this, except we wouldn't be able to use categorical_crossentropy\n",
    "for our loss function. That method requires labels to follow categorical encoding.\n",
    "<br/><br/>\n",
    "Instead, we would use *sparse_categorical_crossentropy* which is the same loss function, it just\n",
    "interacs with the data differently.\n",
    "### 3.5.7 The importance of having sufficiently large intermediate layers\n",
    "What would happen if we had layers with dimensionality smaller than our final output? As mentioned\n",
    "earlier, it would create an \"information bottleneck\". Let's see what that would do to our\n",
    "model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model with an information bottleneck\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(4, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(partialXtrain,\n",
    "          partialYtrain,\n",
    "          epochs=20,\n",
    "          batch_size=512,\n",
    "          validation_data=(xVal, yVal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a 70.2% accuracy on the validation data, a nearly 10% absolute drop from our initial model.\n",
    "### 3.5.8 Further Experiments\n",
    "* Try using larger or smaller layers\n",
    "* Try a single or three hidden layers\n",
    "## Key Takeaways from This Example\n",
    "* When you have N classes to categorize into, your ouput layer should be a Dense layer of size N\n",
    "* If the problem is single-layer, multiclass, then the output layer should use *softmax* activation\n",
    "* Categorical crossentropy is nearly always the correst loss function for this type of problem\n",
    "* Labels can either be case as integers (loss function becomes sparse categorical crossentropy) or one hot encoded\n",
    "* Avoid creating information bottlenecks: make sure the hidden layers are big enough so that info isn't dropped\n",
    "## 3.6 Prediction house prices: a regression example\n",
    "### 3.6.1 The Boston Housing Prices dataset\n",
    "We'll attempt to predict the median price of homes in Boston suburbs in the mid-70s. Unlike the previous\n",
    "examples, this dataset has relatively few data points: 506, split between 404 for training and 102 for testing.\n",
    "<br/><br/>\n",
    "Also, each feature in the dataset (e.g. crime rate) has a different scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "#Create train and test data\n",
    "(trainData, trainTargets), (testData, testTargets) = boston_housing.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Preparing the Data\n",
    "Neural networks want data that is homogeneous-- not in different scales. As such, we'll perform some\n",
    "*feature-wise normalization*: for each feature, we'll subtract the mean of the feature and divide by\n",
    "the standard deviation. This way the feature is centered around zero and has a *unit standard deviation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using numpy to normalize the data\n",
    "import numpy as np\n",
    "\n",
    "mean = trainData.mean(axis = 0)\n",
    "trainData -= mean\n",
    "std = trainData.std(axis = 0)\n",
    "trainData /= std\n",
    "\n",
    "testData -= mean\n",
    "testData /= std\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Note: The training mean and std was used on the test data. Never compute **anything** on the test data!*\n",
    "<br/><br/>\n",
    "### 3.6.3 Building your network\n",
    "Since we have so few examples in our data, we'll create a small network: 2 hidden layers, 64 hidden units each.\n",
    "Overfitting is always an issue with small training sets, so we can try and mitigate that a bit by\n",
    "training a smal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def buildModel(): #defining the model as a function that we can call\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation = 'relu', input_shape = (trainData.shape[1], )))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer = 'rmsprop',\n",
    "                  loss = 'mse',\n",
    "                  metrics = ['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes on the above model specification:\n",
    "* The output layer has no activation because we are predicting linear values. Activation functions would limit its range.\n",
    "* We're using mean squared error as our loss function.\n",
    "* Our metric is *mean absolute error*. We want to minimize the absolute value between the prediction and targets\n",
    "### 3.6.4 Validating your approach using K-fold cross-validation\n",
    "Since we have so few data points, simply splitting into a train and validation set can cause problems. Namely, our\n",
    "validation scores might not be a good predictor of performance on our test data. There can be a high variance between\n",
    "validation scores depending on which part of the small dataset gets set aside.\n",
    "<br/><br/>\n",
    "To deal with this, we'll use K-fold cross-validation. We;ll split the available data in K partitions,\n",
    "and run the model on each K-1. Then, we average the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model with K-fold validation\n",
    "\n",
    "k = 4\n",
    "numValSamples = len(trainData) // k #Determining our fold size\n",
    "numEpochs = 100\n",
    "\n",
    "allScores = [] #We'll place our score after each iteration of the k-fold here\n",
    "\n",
    "for i in range(k): #for each of our k-fold iterations\n",
    "    print('processing fold #', i)\n",
    "    valData = trainData[i * numValSamples: (i + 1) * numValSamples] #creating our data partition\n",
    "    valTargets = trainTargets[i * numValSamples : (i + 1) * numValSamples]\n",
    "\n",
    "    partialTrainData = np.concatenate(\n",
    "        [trainData[:i * numValSamples],\n",
    "        trainData[(i + 1) * numValSamples:]],\n",
    "        axis = 0)\n",
    "    partialTrainTargets = np.concatenate(\n",
    "        [trainTargets[:i * numValSamples],\n",
    "        trainTargets[(i + 1) * numValSamples:]],\n",
    "        axis = 0)\n",
    "    \n",
    "    model = buildModel()\n",
    "    model.fit(partialTrainData, partialTrainTargets,\n",
    "              epochs = numEpochs, batch_size=1, verbose=0)\n",
    "    valMSE, valMAE = model.evaluate(valData, valTargets, verbose=0)\n",
    "    allScores.append(valMAE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking MAE scores and findings the mean\n",
    "print(allScores)\n",
    "\n",
    "print(np.mean(allScores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, we're off by about $2,400 on each prediction. Since most house prices are < $50K, that's a bit wide.\n",
    "<br/><br/>\n",
    "As seen above, we're off on our predictions by ~$2,400...a pretty big gap considering how low prices are.\n",
    "Let's modify our code slightly so that we train across 500 epochs instead of 100, and track performance of each epoch\n",
    "by saving the per-epoch validation score log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the validation logs at each fold\n",
    "numEpochs = 500\n",
    "allMAEhistories = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    valData = trainData[i * numValSamples: (i + 1) * numValSamples]\n",
    "    valTargets = trainTargets[i * numValSamples: (i + 1) * numValSamples]\n",
    "    partialTrainData = np.concatenate(\n",
    "        [trainData[:i * numValSamples],\n",
    "        trainData[(i + 1) * numValSamples:]],\n",
    "        axis = 0)\n",
    "    partialTrainTargets = np.concatenate(\n",
    "        [trainTargets[:i * numValSamples],\n",
    "        trainTargets[(i + 1) * numValSamples:]],\n",
    "        axis = 0)\n",
    "    \n",
    "    model = buildModel()\n",
    "    history = model.fit(partialTrainData, partialTrainTargets,\n",
    "                        validation_data=(valData, valTargets),\n",
    "                        epochs=numEpochs, batch_size=1, verbose=0)\n",
    "    maeHistory = history.history['val_mean_absolute_error']\n",
    "    allMAEhistories.append(maeHistory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll compute the history of successive mean K-fold scores\n",
    "\n",
    "averageMaeHistory = [\n",
    "    np.mean([x[i] for x in allMAEhistories]) for i in range(numEpochs)]\n",
    "\n",
    "# and plot the results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(averageMaeHistory) + 1), averageMaeHistory)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That chart was a bit tough to read, so we'll do a few things:\n",
    "# * Omit the first 10 data points, to get the chart's scale a bit more readable \n",
    "# * Replace the points with an exponential moving average to smooth out the line\n",
    "\n",
    "def smoothCurve(points, factor = 0.9):\n",
    "    smoothedPoints = []\n",
    "    for point in points:\n",
    "        if smoothedPoints:\n",
    "            previous = smoothedPoints[-1]\n",
    "            smoothedPoints.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothedPoints.append(point)\n",
    "    return smoothedPoints\n",
    "\n",
    "smoothMAEhistory = smoothCurve(averageMaeHistory[10:])\n",
    "\n",
    "plt.plot(range(1, len(smoothMAEhistory) + 1), smoothMAEhistory)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model stops improving after ~80 epochs. So, we can tune our\n",
    "parameters to reflect that in our final model. *Note: we'll also change our batch size to 16,\n",
    "reflecting his final model in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = buildModel()\n",
    "model.fit(trainData, trainTargets,\n",
    "          epochs=80, batch_size=16, verbose=0)\n",
    "testMseScore, testMaeScore = model.evaluate(testData, testTargets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're off by ~$2,900 on average\n",
    "## Key Takeaways from This Example\n",
    "* Regression uses different loss functions (MSE being a popular one)\n",
    "* Evaluation metrics are also different for regression. Mean absolute error (MAE) is common\n",
    "* When features (columns) in the input data have different scales, each feature should be normalized in preprocessing\n",
    "* This can be done by subtracting the feature mean and dividing by the feature std\n",
    "* When the dataset has low observations, K-fold validation can increase evaluation reliability\n",
    "* When observations are low, use a small network with only one or two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
